\documentclass[10pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts,amssymb}

\usepackage{epsfig}

\usepackage{color}
\newcommand{\bl}{\color{blue}}
\newcommand{\re}{\color{red}}
\newcommand{\gr}{\color{green}}
\newcommand{\ul}{\underline}
\newcommand{\mpr}{\marginpar}
\usepackage{ulem}

\usepackage{latexsym}

\addtolength{\textwidth}{1in} \addtolength{\oddsidemargin}{-0.5in}
\addtolength{\textheight}{1.6in} \addtolength{\topmargin}{-0.8in}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newtheorem{definition}[theorem]{Definition}

\renewcommand{\theequation}{\thesection.\arabic{equation}}


\newcommand{\R}{\mathbb{R}}


\thispagestyle{empty} 


\begin{document}

\baselineskip = 20pt plus 3pt minus 3pt

\centerline{\Large \bf Final Project Proposal for STAT 6801} 
\begin{center} By: Julia Harvie, 0916770 \\
November 20, 2020 \end{center}

\bigskip

\section{Introduction}\label{sec:intro}
The dataset of interest was created by concatenating the quality of red wines and the quality of white wines, data sets presented by Cortez et al., 2009\cite {1}. 12 measurements were recorded for each wine sample and a class of either red or white indicated. The objective of this analysis will be to fit a binomial regression model that will predict the colour of the wine based off these sample measurements.

Even though this dataset has comes with 12 measurements per sample there is a possibility that not all of them are need to build an informative model. To test for this LASSO regression will be used. Unlike other regression types such as ridge regression, LASSO does not just shrink coefficients, it can reduce them to zero. However it still needs to be considered that LASSO may not be the most informative regression method for this data set. If the measurements are highly correlated, then ridge regression will outperform LASSO\cite{2}. In consideration of this, the model will be fitted using elastic net regularization.

For this method, instead of using the LASSO penalty to minimize the mean squared error (MSE) during model fitting, the elastic net penalty, $ (1 - \alpha)|\beta|_1+\alpha|\beta|^2 $ will be used\cite{2}. This penalty is an amalgamation of the ridge and the LASSO penalties controlled by the parameter $\alpha$. When $\alpha$ is set to 0 standard ridge regression is performed. When $\alpha$ is set to 1 standard LASSO regression is performed. When $0<\alpha<1$ both regression methods influence the fit, maximizing the benefits each method can provide.  The R package Glmnet will be used to analysis this data set as it allows for the LASSO or the elastic net penalty to be used to fit a generalized linear model\cite{3}.

\section{Body}\label{sec:body}
The first step will be subset the data into training and testing datasets. This will be done using down sampling to ensure an equal representation of each class in both data sets to prevent the introduction of bias through over or under representation. In order to fit a binomial regression model using elastic net a critical $\alpha$ and $\lambda$ must be identified for the data set. Both parameters will require cross validation which can be done using the function cv.glmnet.  Because the data is binomial a critical $\lambda$ can be found that either minimizes MSE or that minimizes misclassification. By default 'cv.glmnet' has an alpha set to 1 meaning only the LASSO penalty is being used. By assigning an $\alpha  < $ 1  the elastic net penalty is used instead. The 'cv.glmnet'  argument foldid will be utilized to perform cross validation of various $\alpha$ values to determine the one that performs the best. Once the critical $\lambda$ and $\alpha$ values have been determined, the function 'glmnet' will be used to fit the model. Glmnet can be used to fit multiple models that span all the value of $\lambda$ from a value which allows every coefficients to $ > $ zero, to a value where all coefficients have been shrunk to zero. Like cv.glmnet, glmnet also has a default $\alpha$ value of 1.  Alternatively the critical $\lambda$ and $\alpha$ values can be set directly in the function to produce a singular model. A singular model will be fit using the $\alpha$ and $\lambda$ parameters determined previously. The coefficients of this model will be examined to determine which features survived the regression. Finally, this model will be applied to the testing dataset and evaluated using a confusion matrix

\section{Conclusion}\label{sec:conclusion}
Through out this analysis addition information can be inferred about the dataset by examining some of the other outputs created by the model fitting functions. By plotting the output of a glmnet fit for all values of $\lambda$ it can be seen the order of which the coefficients shrink to zero, presenting an order of importance. The critical $\alpha$ value will also give insight into how correlated the features are. It has been seen that when features are highly correlated ridge regression outperforms LASSO and as $\alpha$ approach zero the elastic net penalty is favouring the ridge regression component. Therefor a low $\alpha$ value can be indicative of high levels of correlation among some of the features\cite{2} .


\begin{thebibliography}{9}
\bibitem{1} 
Cortez, P., Cerdeira, A., Almeida, F., Matos T., \& Reis., J. (2009)
Modeling wine preferences by data mining from physicochemical properties.
\textit{In Decision Support Systems, Elsevier, 47(4), 547-553}
https://doi.org/10.1016/j.dss.2009.05.016
\bibitem{2} 
Zou, H., \& Hastie, T. (2005)
Regularization and variable selection via the elastic net.
\textit{Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301-320}.
doi:10.1111/j.1467-9868.2005.00503.x
\bibitem{3} 
Hastie, T., Qian, J. (2016)
An Introduction to \bf{glmnet}
\\\texttt{https://cran.r-project.org/web/packages/glmnet/glmnet.pdf}

\end{thebibliography}

\end{document}























